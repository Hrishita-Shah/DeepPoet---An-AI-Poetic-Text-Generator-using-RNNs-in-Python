{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kuy8zvgng2ba",
        "outputId": "e40b8a65-839e-49a7-9f7c-8ae88c632eec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-bf498f00eef5>:29: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  x = np.zeros((len(sentences), SEQ_LENGTH, len(characters)), dtype=np.bool)\n",
            "<ipython-input-1-bf498f00eef5>:30: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  y = np.zeros((len(sentences), len(characters)), dtype=np.bool)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "651/651 [==============================] - 153s 232ms/step - loss: 2.8711\n",
            "Epoch 2/100\n",
            "651/651 [==============================] - 143s 219ms/step - loss: 2.3574\n",
            "Epoch 3/100\n",
            "651/651 [==============================] - 143s 219ms/step - loss: 2.2161\n",
            "Epoch 4/100\n",
            "651/651 [==============================] - 142s 218ms/step - loss: 2.1283\n",
            "Epoch 5/100\n",
            "651/651 [==============================] - 141s 217ms/step - loss: 2.0623\n",
            "Epoch 6/100\n",
            "651/651 [==============================] - 141s 216ms/step - loss: 2.0092\n",
            "Epoch 7/100\n",
            "651/651 [==============================] - 144s 221ms/step - loss: 1.9636\n",
            "Epoch 8/100\n",
            "651/651 [==============================] - 144s 221ms/step - loss: 1.9247\n",
            "Epoch 9/100\n",
            "651/651 [==============================] - 142s 218ms/step - loss: 1.8936\n",
            "Epoch 10/100\n",
            "651/651 [==============================] - 143s 219ms/step - loss: 1.8614\n",
            "Epoch 11/100\n",
            "651/651 [==============================] - 144s 221ms/step - loss: 1.8348\n",
            "Epoch 12/100\n",
            "651/651 [==============================] - 144s 221ms/step - loss: 1.8105\n",
            "Epoch 13/100\n",
            "651/651 [==============================] - 144s 221ms/step - loss: 1.7872\n",
            "Epoch 14/100\n",
            "651/651 [==============================] - 143s 220ms/step - loss: 1.7656\n",
            "Epoch 15/100\n",
            "651/651 [==============================] - 144s 221ms/step - loss: 1.7464\n",
            "Epoch 16/100\n",
            "651/651 [==============================] - 143s 219ms/step - loss: 1.7275\n",
            "Epoch 17/100\n",
            "651/651 [==============================] - 144s 222ms/step - loss: 1.7096\n",
            "Epoch 18/100\n",
            "651/651 [==============================] - 141s 217ms/step - loss: 1.6936\n",
            "Epoch 19/100\n",
            "651/651 [==============================] - 144s 221ms/step - loss: 1.6774\n",
            "Epoch 20/100\n",
            "651/651 [==============================] - 144s 221ms/step - loss: 1.6620\n",
            "Epoch 21/100\n",
            "651/651 [==============================] - 147s 225ms/step - loss: 1.6481\n",
            "Epoch 22/100\n",
            "651/651 [==============================] - 144s 222ms/step - loss: 1.6335\n",
            "Epoch 23/100\n",
            "651/651 [==============================] - 145s 223ms/step - loss: 1.6206\n",
            "Epoch 24/100\n",
            "651/651 [==============================] - 144s 221ms/step - loss: 1.6074\n",
            "Epoch 25/100\n",
            "651/651 [==============================] - 144s 221ms/step - loss: 1.5955\n",
            "Epoch 26/100\n",
            "651/651 [==============================] - 145s 222ms/step - loss: 1.5839\n",
            "Epoch 27/100\n",
            "651/651 [==============================] - 145s 222ms/step - loss: 1.5723\n",
            "Epoch 28/100\n",
            "651/651 [==============================] - 144s 221ms/step - loss: 1.5616\n",
            "Epoch 29/100\n",
            "651/651 [==============================] - 142s 219ms/step - loss: 1.5516\n",
            "Epoch 30/100\n",
            "651/651 [==============================] - 142s 218ms/step - loss: 1.5413\n",
            "Epoch 31/100\n",
            "651/651 [==============================] - 144s 221ms/step - loss: 1.5315\n",
            "Epoch 32/100\n",
            "651/651 [==============================] - 145s 223ms/step - loss: 1.5226\n",
            "Epoch 33/100\n",
            "651/651 [==============================] - 144s 221ms/step - loss: 1.5133\n",
            "Epoch 34/100\n",
            "651/651 [==============================] - 145s 223ms/step - loss: 1.5044\n",
            "Epoch 35/100\n",
            "651/651 [==============================] - 146s 223ms/step - loss: 1.4958\n",
            "Epoch 36/100\n",
            "651/651 [==============================] - 144s 221ms/step - loss: 1.4871\n",
            "Epoch 37/100\n",
            "651/651 [==============================] - 144s 221ms/step - loss: 1.4792\n",
            "Epoch 38/100\n",
            "651/651 [==============================] - 144s 221ms/step - loss: 1.4710\n",
            "Epoch 39/100\n",
            "651/651 [==============================] - 146s 224ms/step - loss: 1.4633\n",
            "Epoch 40/100\n",
            "651/651 [==============================] - 145s 223ms/step - loss: 1.4558\n",
            "Epoch 41/100\n",
            "651/651 [==============================] - 145s 222ms/step - loss: 1.4481\n",
            "Epoch 42/100\n",
            "651/651 [==============================] - 145s 223ms/step - loss: 1.4411\n",
            "Epoch 43/100\n",
            "651/651 [==============================] - 146s 224ms/step - loss: 1.4337\n",
            "Epoch 44/100\n",
            "651/651 [==============================] - 144s 222ms/step - loss: 1.4262\n",
            "Epoch 45/100\n",
            "651/651 [==============================] - 144s 222ms/step - loss: 1.4194\n",
            "Epoch 46/100\n",
            "651/651 [==============================] - 146s 224ms/step - loss: 1.4125\n",
            "Epoch 47/100\n",
            "651/651 [==============================] - 146s 224ms/step - loss: 1.4053\n",
            "Epoch 48/100\n",
            "651/651 [==============================] - 144s 222ms/step - loss: 1.3982\n",
            "Epoch 49/100\n",
            "651/651 [==============================] - 146s 225ms/step - loss: 1.3912\n",
            "Epoch 50/100\n",
            "651/651 [==============================] - 145s 222ms/step - loss: 1.3848\n",
            "Epoch 51/100\n",
            "651/651 [==============================] - 145s 223ms/step - loss: 1.3785\n",
            "Epoch 52/100\n",
            "651/651 [==============================] - 145s 223ms/step - loss: 1.3722\n",
            "Epoch 53/100\n",
            "651/651 [==============================] - 143s 220ms/step - loss: 1.3660\n",
            "Epoch 54/100\n",
            "651/651 [==============================] - 144s 222ms/step - loss: 1.3594\n",
            "Epoch 55/100\n",
            "651/651 [==============================] - 143s 219ms/step - loss: 1.3533\n",
            "Epoch 56/100\n",
            "651/651 [==============================] - 145s 222ms/step - loss: 1.3461\n",
            "Epoch 57/100\n",
            "651/651 [==============================] - 143s 219ms/step - loss: 1.3400\n",
            "Epoch 58/100\n",
            "651/651 [==============================] - 143s 220ms/step - loss: 1.3341\n",
            "Epoch 59/100\n",
            "651/651 [==============================] - 146s 225ms/step - loss: 1.3277\n",
            "Epoch 60/100\n",
            "651/651 [==============================] - 144s 222ms/step - loss: 1.3219\n",
            "Epoch 61/100\n",
            "651/651 [==============================] - 146s 224ms/step - loss: 1.3163\n",
            "Epoch 62/100\n",
            "651/651 [==============================] - 143s 220ms/step - loss: 1.3093\n",
            "Epoch 63/100\n",
            "651/651 [==============================] - 147s 225ms/step - loss: 1.3036\n",
            "Epoch 64/100\n",
            "651/651 [==============================] - 144s 221ms/step - loss: 1.2985\n",
            "Epoch 65/100\n",
            "651/651 [==============================] - 144s 222ms/step - loss: 1.2917\n",
            "Epoch 66/100\n",
            "651/651 [==============================] - 143s 220ms/step - loss: 1.2862\n",
            "Epoch 67/100\n",
            "651/651 [==============================] - 146s 224ms/step - loss: 1.2801\n",
            "Epoch 68/100\n",
            "651/651 [==============================] - 144s 222ms/step - loss: 1.2749\n",
            "Epoch 69/100\n",
            "651/651 [==============================] - 145s 223ms/step - loss: 1.2687\n",
            "Epoch 70/100\n",
            "651/651 [==============================] - 143s 220ms/step - loss: 1.2629\n",
            "Epoch 71/100\n",
            "651/651 [==============================] - 145s 222ms/step - loss: 1.2588\n",
            "Epoch 72/100\n",
            "651/651 [==============================] - 144s 221ms/step - loss: 1.2532\n",
            "Epoch 73/100\n",
            "651/651 [==============================] - 145s 223ms/step - loss: 1.2479\n",
            "Epoch 74/100\n",
            "651/651 [==============================] - 144s 221ms/step - loss: 1.2429\n",
            "Epoch 75/100\n",
            "651/651 [==============================] - 145s 223ms/step - loss: 1.2379\n",
            "Epoch 76/100\n",
            "651/651 [==============================] - 145s 223ms/step - loss: 1.2314\n",
            "Epoch 77/100\n",
            "651/651 [==============================] - 146s 224ms/step - loss: 1.2268\n",
            "Epoch 78/100\n",
            "651/651 [==============================] - 144s 222ms/step - loss: 1.2214\n",
            "Epoch 79/100\n",
            "651/651 [==============================] - 147s 226ms/step - loss: 1.2168\n",
            "Epoch 80/100\n",
            "651/651 [==============================] - 145s 222ms/step - loss: 1.2127\n",
            "Epoch 81/100\n",
            "651/651 [==============================] - 146s 224ms/step - loss: 1.2071\n",
            "Epoch 82/100\n",
            "651/651 [==============================] - 148s 227ms/step - loss: 1.2034\n",
            "Epoch 83/100\n",
            "651/651 [==============================] - 148s 228ms/step - loss: 1.1982\n",
            "Epoch 84/100\n",
            "651/651 [==============================] - 152s 234ms/step - loss: 1.1939\n",
            "Epoch 85/100\n",
            "651/651 [==============================] - 152s 234ms/step - loss: 1.1900\n",
            "Epoch 86/100\n",
            "651/651 [==============================] - 151s 231ms/step - loss: 1.1857\n",
            "Epoch 87/100\n",
            "651/651 [==============================] - 148s 227ms/step - loss: 1.1809\n",
            "Epoch 88/100\n",
            "651/651 [==============================] - 148s 228ms/step - loss: 1.1765\n",
            "Epoch 89/100\n",
            "651/651 [==============================] - 147s 226ms/step - loss: 1.1725\n",
            "Epoch 90/100\n",
            "651/651 [==============================] - 150s 231ms/step - loss: 1.1686\n",
            "Epoch 91/100\n",
            "651/651 [==============================] - 149s 229ms/step - loss: 1.1640\n",
            "Epoch 92/100\n",
            "651/651 [==============================] - 149s 229ms/step - loss: 1.1604\n",
            "Epoch 93/100\n",
            "651/651 [==============================] - 146s 225ms/step - loss: 1.1575\n",
            "Epoch 94/100\n",
            "651/651 [==============================] - 145s 223ms/step - loss: 1.1530\n",
            "Epoch 95/100\n",
            "651/651 [==============================] - 146s 225ms/step - loss: 1.1502\n",
            "Epoch 96/100\n",
            "651/651 [==============================] - 144s 221ms/step - loss: 1.1461\n",
            "Epoch 97/100\n",
            "651/651 [==============================] - 147s 225ms/step - loss: 1.1433\n",
            "Epoch 98/100\n",
            "651/651 [==============================] - 145s 223ms/step - loss: 1.1385\n",
            "Epoch 99/100\n",
            "651/651 [==============================] - 147s 226ms/step - loss: 1.1365\n",
            "Epoch 100/100\n",
            "651/651 [==============================] - 144s 221ms/step - loss: 1.1322\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------0.2----------\n",
            "en are not yet cold under water, nor\n",
            "the!!apiw!ukulpd-wrduld,\n",
            "ra\n",
            "l\n",
            "lrl\n",
            "\n",
            "\n",
            ",\n",
            "\n",
            "ei  \n",
            "----------0.4----------\n",
            "d converts to bad,\n",
            "And thy abundant good,?k!slutlrc!e\n",
            "?ddo,,i,!e'lymgaillnnlall \n",
            "----------0.6----------\n",
            "ousin, I'll\n",
            "Dispose of you.\n",
            "Gentlemen, gr;ETaa!?-uw\n",
            "kdp..ldaide,e!?adloum,ig,dro\n",
            "----------0.8----------\n",
            "ands will fly.\n",
            "Farewell at once, for oncupa'didsutem-\n",
            "!,uypl!Ca!!t! ''o,,olal, e\n",
            "----------1----------\n",
            "atience waiteth on true sorrow.\n",
            "And see u!rn?,!r\n",
            "l!;!?\n",
            "y!r,d!h'?y,rancilo!natl,t\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Activation\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "filepath = tf.keras.utils.get_file('shakespeare.txt','https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "text = open(filepath, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "text = text[300000:800000]\n",
        "\n",
        "characters = sorted(set(text))\n",
        "\n",
        "char_to_index = dict((c,i) for i, c in enumerate(characters))\n",
        "index_to_char = dict((i, c) for i, c in enumerate(characters))\n",
        "\n",
        "SEQ_LENGTH = 40\n",
        "STEP_SIZE = 3\n",
        "\n",
        "sentences = []\n",
        "next_characters = []\n",
        "\n",
        "for i in range(0, len(text) - SEQ_LENGTH, STEP_SIZE):\n",
        "    sentences.append(text[i: i+SEQ_LENGTH])\n",
        "    next_characters.append(text[i+SEQ_LENGTH])\n",
        "\n",
        "x = np.zeros((len(sentences), SEQ_LENGTH, len(characters)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(characters)), dtype=np.bool)\n",
        "\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, character in enumerate(sentence):\n",
        "        x[i, t, char_to_index[character]] = 1\n",
        "    y[i, char_to_index[next_characters[i]]] = 1\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(SEQ_LENGTH, len(characters))))\n",
        "model.add(Dense(len(characters)))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.01))\n",
        "\n",
        "model.fit(x, y, batch_size=256, epochs=100)\n",
        "\n",
        "model.save('textgenerator.model')\n",
        "\n",
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "def generate_text(length, temperature):\n",
        "    start_index = random.randint(0, len(text) - SEQ_LENGTH - 1)\n",
        "    generated = ''\n",
        "    sentence = text[start_index: start_index + SEQ_LENGTH]\n",
        "    generated += sentence\n",
        "    for i in range(length):\n",
        "        x = np.zeros((1, SEQ_LENGTH, len(characters)))\n",
        "        for t, character in enumerate(sentence):\n",
        "            x[0, t, char_to_index[character]] = 1\n",
        "\n",
        "            predictions = model.predict(x, verbose=0)[0]\n",
        "            next_index = sample(predictions, temperature)\n",
        "            next_characters = index_to_char[next_index]\n",
        "\n",
        "            generated += next_characters\n",
        "            sentence = sentence[1:] + next_characters\n",
        "\n",
        "        return generated\n",
        "\n",
        "print('----------0.2----------')\n",
        "print(generate_text(300, 0.2))\n",
        "print('----------0.4----------')\n",
        "print(generate_text(300, 0.4))\n",
        "print('----------0.6----------')\n",
        "print(generate_text(300, 0.6))\n",
        "print('----------0.8----------')\n",
        "print(generate_text(300, 0.8))\n",
        "print('----------1----------')\n",
        "print(generate_text(300, 1.0))\n",
        "\n",
        "\n"
      ]
    }
  ]
}